{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-30T10:12:59.995309Z",
     "start_time": "2024-05-30T10:12:59.554301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 0.6928\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_derivative() missing 1 required positional argument: 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 113\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;66;03m# Create and train the neural network\u001B[39;00m\n\u001B[0;32m    112\u001B[0m nn \u001B[38;5;241m=\u001B[39m NeuralNetwork(input_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, hidden_size1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, hidden_size2\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, output_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m--> 113\u001B[0m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_one_hot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 97\u001B[0m, in \u001B[0;36mNeuralNetwork.train\u001B[1;34m(self, X, y_true, epochs, learning_rate)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m---> 97\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 68\u001B[0m, in \u001B[0;36mNeuralNetwork.backward\u001B[1;34m(self, X, y_true, learning_rate)\u001B[0m\n\u001B[0;32m     66\u001B[0m dA2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(dZ3, params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mW3\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mT)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# dZ2 = dA2 * relu_derivative(params['Z2'])\u001B[39;00m\n\u001B[1;32m---> 68\u001B[0m dZ2 \u001B[38;5;241m=\u001B[39m dA2 \u001B[38;5;241m*\u001B[39m \u001B[43mcross_entropy_derivative\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mZ2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m dW2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mT, dZ2) \u001B[38;5;241m/\u001B[39m m\n\u001B[0;32m     70\u001B[0m db2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dZ2, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m/\u001B[39m m\n",
      "\u001B[1;31mTypeError\u001B[0m: cross_entropy_derivative() missing 1 required positional argument: 'y_pred'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    exp_x = np.exp(x - np)\n",
    "    \n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    n_samples = y_true.shape[0]\n",
    "    log_p = - np.log(y_pred[range(n_samples), y_true.argmax(axis=1)])\n",
    "    loss = np.sum(log_p) / n_samples\n",
    "    return loss\n",
    "\n",
    "# Cross-entropy is the COST function. The derivate is according to one-hot encoded labels...\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# Initialize the neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        self.params = {\n",
    "            \"W1\": np.random.randn(input_size, hidden_size1) * 0.01,\n",
    "            \"b1\": np.zeros((1, hidden_size1)),\n",
    "            \"W2\": np.random.randn(hidden_size1, hidden_size2) * 0.01,\n",
    "            \"b2\": np.zeros((1, hidden_size2)),\n",
    "            \"W3\": np.random.randn(hidden_size2, output_size) * 0.01,\n",
    "            \"b3\": np.zeros((1, output_size))\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        params = self.params\n",
    "\n",
    "        # First layer\n",
    "        params['Z1'] = np.dot(X, params['W1']) + params['b1']\n",
    "        # params['A1'] = relu(params['Z1'])\n",
    "        params['A1'] = softmax(params['Z1'])\n",
    "\n",
    "        # Second layer\n",
    "        params['Z2'] = np.dot(params['A1'], params['W2']) + params['b2']\n",
    "        # params['A2'] = relu(params['Z2'])\n",
    "        params['A2'] = softmax(params['Z2'])\n",
    "\n",
    "        # Output layer\n",
    "        params['Z3'] = np.dot(params['A2'], params['W3']) + params['b3']\n",
    "        params['A3'] = softmax(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "\n",
    "    def backward(self, X, y_true, learning_rate):\n",
    "        params = self.params\n",
    "        m = y_true.shape[0]\n",
    "\n",
    "        # Output layer gradients\n",
    "        dZ3 = cross_entropy_derivative(y_true, params['A3'])    \n",
    "        dW3 = np.dot(params['A2'].T, dZ3) / m\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Second layer gradients\n",
    "        dA2 = np.dot(dZ3, params['W3'].T)\n",
    "        # dZ2 = dA2 * relu_derivative(params['Z2'])\n",
    "        dZ2 = dA2 * cross_entropy_derivative(params['Z2'])\n",
    "        dW2 = np.dot(params['A1'].T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # First layer gradients\n",
    "        dA1 = np.dot(dZ2, params['W2'].T)\n",
    "        # dZ1 = dA1 * relu_derivative(params['Z1'])\n",
    "        dZ1 = dA1 * cross_entropy_derivative(params['Z1'])\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update parameters\n",
    "        params['W1'] -= learning_rate * dW1\n",
    "        params['b1'] -= learning_rate * db1\n",
    "        params['W2'] -= learning_rate * dW2\n",
    "        params['b2'] -= learning_rate * db2\n",
    "        params['W3'] -= learning_rate * dW3\n",
    "        params['b3'] -= learning_rate * db3\n",
    "\n",
    "    def train(self, X, y_true, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(y_true, y_pred)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(X, y_true, learning_rate)\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 3)  # 100 samples, 3 features\n",
    "y_train = np.random.randint(0, 2, 100)  # 100 samples, binary classification\n",
    "y_train_one_hot = one_hot_encode(y_train, 2)\n",
    "\n",
    "# Create and train the neural network\n",
    "nn = NeuralNetwork(input_size=3, hidden_size1=5, hidden_size2=4, output_size=2)\n",
    "nn.train(X_train, y_train_one_hot, epochs=1000, learning_rate=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
